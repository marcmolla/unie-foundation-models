{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c79cf0f1",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3219e0f8",
   "metadata": {},
   "source": [
    "The Lab. 1 is an introduction to the laboratories environment for the subject Foundation Models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b4cf1d",
   "metadata": {},
   "source": [
    "# Kaggle environment\n",
    "\n",
    "A tour through Kaggle will be shown during the first session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835de634",
   "metadata": {},
   "source": [
    "# Local Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa7912c",
   "metadata": {},
   "source": [
    "It is possible to work directly on Kaggle but it is recommended to  create a local environment and then you can use Visual Studio Code to work with the notebooks. The `local_requirements.txt` contains the requirements for the local environment, and uses the same packages version that Kaggle.\n",
    "\n",
    "Please, create a local environment (`F1 -> Python Create Environment...`) and apply the requirements:\n",
    "```bash\n",
    "pip install -r local_requirements.txt\n",
    "```\n",
    "\n",
    "Please, select Python 3.12 to keep compatibility with Kaggle.\n",
    "\n",
    "> **Note** The default installation uses Torch only compatible with CPU. If you are using a Mac with Apple silicon, `mps` will be used automatically. For Linux or Windows/WSL, it is recommended to install CUDA 12.8 and the correspondent version of torch. If you do not have GPU, just run on local the simple examples and use Kaggle for the heavy workload (e.g. fine-tunning, inference with huge models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231a945",
   "metadata": {},
   "source": [
    "# Use your first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ffaa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model_id)\n",
    "\n",
    "pipe(\"Hello, world!\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f39e122",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "The *transformers* library by [Hugging Face](https://huggingface.co) is a Python package that provides state-of-the-art implementations of Transformer-based models for natural language processing (NLP), computer vision, and multimodal tasks. It allows developers and researchers to load pretrained models, fine-tune them on custom datasets, and deploy them for inference. The library supports a wide range of Transformer architectures, including BERT, GPT, RoBERTa, T5, and many others, covering both encoder-only, decoder-only, and encoder-decoder models.\n",
    "\n",
    "One of the key features of *transformers* is its high-level API for text, image, and audio tasks. With just a few lines of code, users can perform text generation, classification, question answering, summarization, translation, and token-level tasks. The library also integrates seamlessly with popular deep learning frameworks such as PyTorch and TensorFlow, offering both flexibility and performance.\n",
    "\n",
    "In addition to model implementations, *transformers* provides tokenizers, which handle preprocessing text into tokens suitable for the models, supporting techniques such as WordPiece, Byte-Pair Encoding (BPE), and SentencePiece. The library also includes training utilities, pipelines, and ready-to-use pretrained models through the Hugging Face Model Hub, making it possible to leverage zero-shot, few-shot, and transfer learning without building models from scratch.\n",
    "\n",
    "Overall, the *transformers* library is designed to accelerate NLP research and application development by providing reliable, scalable, and easy-to-use implementations of modern Transformer models. It has become a standard tool in the AI ecosystem for anyone working with language, text, or multimodal data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1111c134",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "Each model has an entry at Hugging Face. For the microsoft Phi-3 model you can check the information in [microsoft/Phi-3-mini-128k-instruct](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct).\n",
    "\n",
    "The most interesting is the Model Card, were you can read informationa about the model and how to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b829b27",
   "metadata": {},
   "source": [
    "### Model name\n",
    "\n",
    "Model names folows a convention to describe them. For example:\n",
    "\n",
    "```\n",
    "microsoft / Phi - 3 - mini - 128k - instruct\n",
    "    |       |     |    |      |      |\n",
    "   Org   Family  Ver. Size Context  Task\n",
    "```\n",
    "Where:\n",
    " * Org is the organization providing the model, in this case Microsoft\n",
    " * Family represents the model family or architecture\n",
    " * Version states the version of the model\n",
    " * Size is the scale of the model (small, medium, etc.)\n",
    " * Context is the size of tokens for the context\n",
    " * Task is how the model was trained (in this case, to follow instructions) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f858a88f",
   "metadata": {},
   "source": [
    "## pipeline\n",
    "\n",
    "*pipeline* is an easy way to use models for quick inference, as it abstracts most of the complexity code from transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0420998",
   "metadata": {},
   "source": [
    "# Chat format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e4f30",
   "metadata": {},
   "source": [
    "One useful information that use to be in the Model Card is the Chat format. It is important to follow it, as the model was fine-tunned (instructed in our case) with that format. For example, for Phi-3 we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b6a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|system|>\n",
    "You are a helpful travel assistant.<|end|>\n",
    "<|user|>\n",
    "I am going to Paris, what should I see?<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "answer = pipe(prompt, max_new_tokens=500)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bda386",
   "metadata": {},
   "source": [
    "And we can parse the answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f9cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answer = answer[0]['generated_text'].split(\"<|assistant|>\")[1]\n",
    "print(model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bcc08f",
   "metadata": {},
   "source": [
    "Each model has its own way of formatting the instructions and outputs, but this is also managed by `transformers`. Let's define the prompt in a transformer's standard way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc5deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful travel assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"I am going to Paris, what should I see?\"}\n",
    "]\n",
    "output = pipe(messages, max_new_tokens=500)\n",
    "print(output[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34204c2",
   "metadata": {},
   "source": [
    "In general, we have:\n",
    " * System role: General instructions where are defined the expectations and for example, the input output format, the kind of the task\n",
    " * User role: Contains the task itself, a question in our example\n",
    " * Assistant role: model output\n",
    "\n",
    "The messages can be concatenated, and are the basis for a context and/or few-shots prompting. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726af920",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful travel assistant.'},\n",
    "    {'role': 'user', 'content': 'I am going to Paris, what should I see?'},\n",
    "    {'role': 'assistant', 'content': \" Paris is a city that offers an abundance of attractions, each with its own unique charm. Here's a list of must-see places that will allow you to experience the essence of Paris:\\n\\n1. Eiffel Tower: Start your trip with a visit to the iconic Eiffel Tower. Take the elevator to the top for a breathtaking panoramic view of the city.\\n\\n2. Louvre Museum: Home to the famous Mona Lisa and thousands of other masterpieces, the Louvre is a must-visit for art lovers.\\n\\n3. Notre-Dame Cathedral: Although currently closed for renovations, the Cathedral is an architectural marvel and will be reopened soon. It's still worth taking a walk around the area to admire its beauty.\\n\\n4. Arc de Triomphe: Located at the western end of the Champs-Élysées, the Arc de Triomphe honors those who fought and died for France. The view from the top is spectacular during sunset.\\n\\n5. Montmartre: This charming neighborhood offers a glimpse into Paris's bohemian past and is known for the Sacré-Cœur Basilica and the vibrant street art.\\n\\n6. Palace of Versailles: A short train ride from central Paris, the Palace of Versailles is a grandiose symbol of absolute monarchy. Explore its opulent rooms and gardens.\\n\\n7. Seine River Cruise: See Paris's most famous landmarks, including the Eiffel Tower, Notre-Dame, and Louvre, from the serene waters of the Seine River.\\n\\n8. Latin Quarter: This historic district is famous for its bohemian atmosphere, lively cafes, and quaint bookshops.\\n\\n9. Champs-Élysées: Take a stroll down this iconic avenue, which is lined with theaters, cafes, and luxury shops.\\n\\n10. Musée d'Orsay: Housed in a former railway station, this museum showcases an extensive collection of Impressionist and Post-Impressionist masterpieces.\\n\\n11. Sainte-Chapelle: Known for its st\"},\n",
    "    {'role': 'user', \"content\": \"Interesting, could you tell me more about Notre Dame Cathedral?\"}\n",
    "]\n",
    "output = pipe(messages, max_new_tokens=500)\n",
    "print(output[0]['generated_text']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b7ba9",
   "metadata": {},
   "source": [
    "# Assignment #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359eb551",
   "metadata": {},
   "source": [
    "The first assignment is to build a Q&A system, using the methods we described above. The requirements are:\n",
    " * Keep it simple: we just need a function to send a query and get a response.\n",
    " * Keep the context in all interactions (> Hint: `output[0]['generated_text]`can be used to create the next input)\n",
    " * Test you system with several related questions about the topic of your interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4725119",
   "metadata": {},
   "source": [
    "# Working with Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8565b8b8",
   "metadata": {},
   "source": [
    "## Integration with Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c203419",
   "metadata": {},
   "source": [
    "If the local environment is created correctly, you will have the command `kaggle`, which allows you to interact with Kaggle:\n",
    "\n",
    "```bash\n",
    "% kaggle -h\n",
    "usage: kaggle [-h] [-v] [-W] {competitions,c,datasets,d,kernels,k,models,m,files,f,config} ...\n",
    "\n",
    "options:\n",
    "  -h, --help            show this help message and exit\n",
    "  -v, --version         Print the Kaggle API version\n",
    "  -W, --no-warn         Disable out-of-date API version warning\n",
    "\n",
    "commands:\n",
    "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
    "                        Use one of:\n",
    "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
    "                        datasets {list, files, download, create, version, init, metadata, status}\n",
    "                        kernels {list, files, init, push, pull, output, status}\n",
    "                        models {instances, get, list, init, create, delete, update}\n",
    "                        models instances {versions, get, files, init, create, delete, update}\n",
    "                        models instances versions {init, create, download, delete, files}\n",
    "                        config {view, set, unset}\n",
    "    competitions (c)    Commands related to Kaggle competitions\n",
    "    datasets (d)        Commands related to Kaggle datasets\n",
    "    kernels (k)         Commands related to Kaggle kernels\n",
    "    models (m)          Commands related to Kaggle models\n",
    "    files (f)           Commands related files\n",
    "    config              Configuration settings\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ea903",
   "metadata": {},
   "source": [
    "You will need to authorize the access of the tool to the Kaggle's API. For that, go to your [kaggle's account](https://www.kaggle.com/settings/account) and create a **Legacy API Credential**.\n",
    "\n",
    "![Legacy API Credentials](./img/lab1/legacy_api.png \"Legacy API Credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78acdc74",
   "metadata": {},
   "source": [
    "Move the downloaded file ('kaggle.json') to a directory '.kaggle' in your home:\n",
    "```bash\n",
    "(.venv)  ~ % mkdir ~/.kaggle/\n",
    "(.venv)  ~ % mv ~/Downloads/kaggle.json ~/.kaggle \n",
    "(.venv)  ~ % chmod 600 ~/.kaggle/kaggle.json\n",
    "(.venv)  ~ % kaggle competitions list   \n",
    "ref                                                                              deadline             category                reward  teamCount  userHasEntered  \n",
    "-------------------------------------------------------------------------------  -------------------  ---------------  -------------  ---------  --------------  \n",
    "https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-3    2026-04-15 23:59:00  Featured         2,207,152 Usd       1535            True  \n",
    "https://www.kaggle.com/competitions/vesuvius-challenge-surface-detection         2026-02-13 23:59:00  Research           200,000 Usd        734           False  \n",
    "https://www.kaggle.com/competitions/med-gemma-impact-challenge                   2026-02-24 23:59:00  Featured           100,000 Usd         51           False  \n",
    "https://www.kaggle.com/competitions/csiro-biomass                                2026-01-28 23:59:00  Research            75,000 Usd       3757           False  \n",
    "https://www.kaggle.com/competitions/stanford-rna-3d-folding-2                    2026-03-25 23:59:00  Featured            75,000 Usd        448           False  \n",
    "https://www.kaggle.com/competitions/santa-2025                                   2026-01-30 23:59:00  Featured            50,000 Usd       3281           False  \n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fa7218",
   "metadata": {},
   "source": [
    "## Upload and run a kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ffcc59",
   "metadata": {},
   "source": [
    "If you prefer to work on local, it is possible to upload and run a notebook from your computer. \n",
    "\n",
    "First, from the directory where you have the notebook, `init`a new notebook:\n",
    "```bash\n",
    "(.venv) src % kaggle kernels init\n",
    "Kernel metadata template written to: ./VisualStudioProjects/unie-foundation-models/src/kernel-metadata.json\n",
    "```\n",
    "The commands creates a new metadata for a kernel. Edit it, and put the information of the `Lab 1` notebook:\n",
    "```json\n",
    "{\n",
    "  \"id\": \"<put your kaggle user here>/introduction\",\n",
    "  \"title\": \"Introduction\",\n",
    "  \"code_file\": \"Lab 1. Introduction.ipynb\",\n",
    "  \"language\": \"python\",\n",
    "  \"kernel_type\": \"notebook\",\n",
    "  \"is_private\": \"true\",\n",
    "  \"enable_gpu\": \"true\",\n",
    "  \"enable_tpu\": \"false\",\n",
    "  \"enable_internet\": \"true\",\n",
    "  \"dataset_sources\": [],\n",
    "  \"competition_sources\": [],\n",
    "  \"kernel_sources\": [],\n",
    "  \"model_sources\": []\n",
    "}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5b3562",
   "metadata": {},
   "source": [
    "> Note: Please check the GPU quota when you activate it, it is limited and maybe you will need it for other labs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c5a2f9",
   "metadata": {},
   "source": [
    "You can upload and run the kernel using push option:\n",
    "```bash\n",
    "(.venv) src % kaggle kernels push \n",
    "Kernel version 1 successfully pushed.  Please check progress at https://www.kaggle.com/code/marcmolla/introduction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8df295",
   "metadata": {},
   "source": [
    "To check the status of the notebook:\n",
    "```bash\n",
    "(.venv) src % kaggle kernels status introduction\n",
    "introduction has status \"KernelWorkerStatus.RUNNING\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310f9467",
   "metadata": {},
   "source": [
    "You can also go to the notebook link and see the Logs:\n",
    "![Logs](./img/lab1/logs.png \"Logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd127f26",
   "metadata": {},
   "source": [
    "After a while (around 3 minutes), you will have the results in the `Notebook` section:\n",
    "![Results](./img/lab1/results.png \"Results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ba551f",
   "metadata": {},
   "source": [
    "## Download kernel and logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfe607a",
   "metadata": {},
   "source": [
    ">Note: Be careful, this will override your work with current kaggle's version!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cb2ac7",
   "metadata": {},
   "source": [
    "```bash\n",
    "(.venv) src % kaggle kernels output 1-introduction\n",
    "Kernel log downloaded to ./unie-foundation-models/src/1-introduction.log \n",
    "(.venv) src % kaggle kernels pull 1-introduction  \n",
    "Source code downloaded to ./unie-foundation-models/src/Lab 1. Introduction.ipynb\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
